# Uczenie Maszynowe w Analizie Danych â€“ Projekt 2025Z

## Lab 4: PorÃ³wnanie modeli i baselineâ€™u. Przygotowanie wynikÃ³w i wnioskÃ³w w formie raportu.

### Przygotowanie wynikÃ³w i wnioskÃ³w w formie raportu

W procesie uczenia maszynowego nie wystarczy jedynie zbudowaÄ‡ model â€” konieczne jest **krytyczne porÃ³wnanie wynikÃ³w** uzyskanych przez rÃ³Å¼ne algorytmy i zestawienie ich z tzw. **baselineâ€™em**, czyli prostÄ…, referencyjnÄ… metodÄ…, ktÃ³ra stanowi punkt odniesienia. Celem tej czÄ™Å›ci zajÄ™Ä‡ jest zrozumienie, jak interpretowaÄ‡ wyniki, jak je porÃ³wnywaÄ‡ oraz jak tworzyÄ‡ raport, ktÃ³ry prezentuje caÅ‚y proces w sposÃ³b zrozumiaÅ‚y i transparentny.

---

### 1. Czym jest baseline?

Baseline to **prosty model referencyjny**, ktÃ³ry ustala minimalny poziom jakoÅ›ci, jaki powinien osiÄ…gnÄ…Ä‡ bardziej zaawansowany model.
DziÄ™ki niemu wiemy, czy nasze modele sÄ…:

* faktycznie uÅ¼yteczne,
* czy moÅ¼e tylko *wyglÄ…dajÄ… na skomplikowane, ale nie przewyÅ¼szajÄ… przypadkowego zgadywania*.

#### Typowe baseline'y:

##### ğŸ”¹ **Dla klasyfikacji binarnej:**

* przewidywanie zawsze wiÄ™kszoÅ›ciowej klasy,
* losowy klasyfikator (random guess),
* prosty model logistyczny z domyÅ›lnymi parametrami,
* model â€zero ruleâ€ (ZeroR): przewiduje zawsze najczÄ™stszy wynik.

##### ğŸ”¹ **Dla klasyfikacji wieloklasowej:**

* przewidywanie najczÄ™stszej klasy,
* losowy wybÃ³r klasy z wagami proporcjonalnymi do czÄ™stoÅ›ci.

#### ğŸ”¹ Po co baseline?

* pozwala oceniÄ‡, czy model poprawia wyniki wzglÄ™dem â€naiwnegoâ€ podejÅ›cia,
* umoÅ¼liwia porÃ³wnanie z modelami zaawansowanymi,
* daje kontekst dla metryk â€” accuracy 85% moÅ¼e byÄ‡ Å›wietne, ale jeÅ›li baseline ma 84%, to juÅ¼ niekoniecznie.

---

### 2. PorÃ³wnywanie modeli â€” jakie aspekty bierzemy pod uwagÄ™?

PorÃ³wnanie modeli to nie tylko spojrzenie na accuracy. NaleÅ¼y uwzglÄ™dniÄ‡:

* **rÃ³Å¼ne metryki jakoÅ›ci**,
* **stabilnoÅ›Ä‡ modelu**,
* **czas trenowania**,
* **zÅ‚oÅ¼onoÅ›Ä‡**,
* **przeciÄ…Å¼enie (overfitting)** i **niedouczenie (underfitting)**,
* **czy model jest interpretowalny**.

Dobre porÃ³wnanie to takie, ktÃ³re pokazuje **kompromisy** miÄ™dzy modelami.

---

### 3. Metryki uÅ¼ywane przy porÃ³wnaniu modeli klasyfikacyjnych

NajczÄ™Å›ciej stosowane:

#### ğŸ”¹ **Accuracy**

Procent poprawnych klasyfikacji â€” dobre przy zbalansowanych klasach.

#### ğŸ”¹ **Precision i Recall**

SzczegÃ³lnie waÅ¼ne w problemach nierÃ³wnowagi klas.

* Precision â€” ile z przewidzianych pozytywnych przykÅ‚adÃ³w byÅ‚o poprawnych.
* Recall â€” ile z faktycznych pozytywÃ³w udaÅ‚o siÄ™ odnaleÅºÄ‡.

#### ğŸ”¹ **F1-score**

Harmoniczna Å›rednia precision i recall.

#### ğŸ”¹ **Confusion matrix**

Pokazuje dokÅ‚adnie, gdzie pojawiajÄ… siÄ™ pomyÅ‚ki.

#### ğŸ”¹ **ROC AUC**

Wskazuje, jak dobrze model rozrÃ³Å¼nia klasy niezaleÅ¼nie od progu.

#### ğŸ”¹ **Log Loss / Cross Entropy**

Pokazuje jakoÅ›Ä‡ probabilistycznych predykcji.

#### ğŸ”¹ **Balanced Accuracy**

Lepsza miara przy niezbalansowanych klasach.

---

### 4. Jak poprawnie wykonaÄ‡ porÃ³wnanie?

#### 1. **Ustaw identyczne warunki eksperymentu**

* ten sam zbiÃ³r danych,
* ten sam podziaÅ‚ trening/test albo ta sama walidacja krzyÅ¼owa,
* identyczny preprocessing danych.

#### 2. **Zbuduj baseline**

Nawet jeÅ›li jest sÅ‚aby â€” musi istnieÄ‡ w eksperymencie.

#### 3. **Naucz kilka rÃ³Å¼nych modeli**

Np.:

* Logistic Regression
* Decision Tree
* Random Forest
* SVM
* kNN
* Naive Bayes
* Perceptron / MLP

#### 4. **Zbieraj metryki**

Najlepiej w formie tabeli, np.:

| Model               | Accuracy | Precision | Recall | F1   | AUC  |
| ------------------- | -------- | --------- | ------ | ---- | ---- |
| Baseline (majority) | 72%      | â€”         | â€”      | â€”    | â€”    |
| Logistic Regression | 85%      | 0.82      | 0.78   | 0.80 | 0.87 |
| Random Forest       | 90%      | 0.88      | 0.86   | 0.87 | 0.93 |
| SVM RBF             | 88%      | 0.85      | 0.84   | 0.84 | 0.91 |

#### 5. **Zadbaj o powtarzalnoÅ›Ä‡**

* ustaw seed (`random_state`),
* opisuj parametry modeli,
* kontroluj losowoÅ›Ä‡ np. w k-fold cross-validation.

---

### 5. Jak przygotowaÄ‡ czytelny raport?

Dobrze przygotowany raport jest kluczowy.
Powinien zawieraÄ‡ **logiczny, spÃ³jny opis caÅ‚ego procesu**, a nie tylko same liczby.

#### Raport powinien mieÄ‡:

#### 1. **Opis zbioru danych**

* wymiar,
* liczba klas,
* opis cech,
* wystÄ™powanie brakÃ³w danych.

#### 2. **Cel analizy**

* Co model ma klasyfikowaÄ‡?
* Jakie metryki sÄ… kluczowe?

#### 3. **Opis baselineâ€™u**

* co wybrano jako baseline i dlaczego,
* jakie osiÄ…gnÄ…Å‚ metryki.

#### 4. **Wyniki kaÅ¼dego modelu**

* w tabeli i/lub wykresach,
* opisowo: co dziaÅ‚a dobrze a co Åºle.

#### 5. **WpÅ‚yw tuningu hiperparametrÃ³w**

* czy tuning poprawiÅ‚ wyniki?
* o ile?

#### 6. **Wizualizacje**

* confusion matrix dla najlepszego modelu,
* ROC curves, jeÅ›li klasyfikacja binarna,
* bar chart z wynikami modeli.

#### 7. **Wnioski koÅ„cowe**

PrzykÅ‚adowe punkty:

* ktÃ³ry model jest najlepszy i dlaczego,
* gdzie model siÄ™ myli,
* czy wyniki sÄ… wystarczajÄ…ce dla zastosowaÅ„ praktycznych,
* co moÅ¼na poprawiÄ‡ (dalszy tuning, inne cechy, wiÄ™kszy zbiÃ³r).

---

### 7. Jak formuÅ‚owaÄ‡ wartoÅ›ciowe wnioski?

Zamiast pisaÄ‡:

> Random Forest jest najlepszy.

Napisz:

> Random Forest uzyskaÅ‚ najwyÅ¼szy F1-score (0.87), co sugeruje, Å¼e dobrze radzi sobie z nierÃ³wnowagÄ… klas.
> Model ma niski bÅ‚Ä…d na zbiorze testowym, a krzywa ROC wskazuje duÅ¼Ä… zdolnoÅ›Ä‡ separacji klas.
> Ograniczeniem jest natomiast sÅ‚aba interpretowalnoÅ›Ä‡.

---

### Podsumowanie

W tej czÄ™Å›ci zajÄ™Ä‡ studenci powinni:

* stworzyÄ‡ baseline baseline jako klasyfikator losowy:


>NajÅ‚atwiejszy sposÃ³b to uÅ¼ycie gotowego narzÄ™dzia z **scikit-learn**:
`DummyClassifier`.
>
>Pozwala on zdefiniowaÄ‡ baseline jako:
>
>* klasyfikator przewidujÄ…cy **losowo**,
>* klasyfikator przewidujÄ…cy **zawsze wiÄ™kszoÅ›ciowÄ… klasÄ™**,
>* klasyfikator **stratified** (losowo z zachowaniem proporcji klas).
>
>
>#### 1. Losowy klasyfikator (zupeÅ‚nie losowy)
>
>```python
>from sklearn.dummy import DummyClassifier
>from sklearn.model_selection import train_test_split
>from sklearn.metrics import accuracy_score
>
># ZaÅ‚Ã³Å¼my Å¼e X, y to TwÃ³j dataset
>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
>
># Baseline: losowe przewidywanie
>baseline_random = DummyClassifier(strategy="uniform", random_state=42)
>baseline_random.fit(X_train, y_train)
>
>y_pred = baseline_random.predict(X_test)
>
>print("Accuracy (losowy baseline):", accuracy_score(y_test, y_pred))
>```
>
>##### Co robi `strategy="uniform"`?
>
>* Model wybiera **kaÅ¼dÄ… klasÄ™ z jednakowym prawdopodobieÅ„stwem**.
>* JeÅ›li masz 3 klasy â†’ kaÅ¼da ma 33% szans.
>
>To najprostszy moÅ¼liwy baseline.
>
>---
>
>####  2. Losowy baseline z zachowaniem proporcji klas (bardziej sensowny)
>
>```python
>baseline_strat = DummyClassifier(strategy="stratified", random_state=42)
>baseline_strat.fit(X_train, y_train)
>
>y_pred = baseline_strat.predict(X_test)
>
>print("Accuracy (baseline stratified):", accuracy_score(y_test, y_pred))
>```
>
>##### Co robi `strategy="stratified"`?
>
>* Losuje klasy **zgodnie z rozkÅ‚adem danych treningowych**.
>* JeÅ›li 80% danych to klasa 0, a 20% to klasa 1 â†’ model bÄ™dzie losowaÅ‚ 80/20.
>
>To najlepszy baseline dla modeli uczÄ…cych siÄ™ na niezbalansowanych danych.
>
>---
>
>#### 3. Baseline przewidujÄ…cy zawsze wiÄ™kszoÅ›ciowÄ… klasÄ™
>
>Warto dodaÄ‡ â€” to najczÄ™Å›ciej stosowany baseline:
>
>```python
>baseline_majority = DummyClassifier(strategy="most_frequent")
>baseline_majority.fit(X_train, y_train)
>
>y_pred = baseline_majority.predict(X_test)
>
>print("Accuracy (most frequent):", accuracy_score(y_test, y_pred))
>```
>
>---
>
>#### 4. Wypisanie peÅ‚nych metryk dla baseline'u
>
>```python
>from sklearn.metrics import classification_report
>
>print(classification_report(y_test, y_pred))
>```

* porÃ³wnaÄ‡ swoje modele z baseline'm,
* przeanalizowaÄ‡ metryki,
* wykonywaÄ‡ tuning hiperparametrÃ³w,
* przygotowaÄ‡ czytelny, naukowy raport,
* wybraÄ‡ "najlepszy" model.